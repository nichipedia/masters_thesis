\section{Results Discussion}
\label{sec:discussion}
\setlength{\parindent}{10ex}
This chapter explores the interesting concepts and emergent ideas that stem from this work.
It is divided into a discussion of the three core experiments performed, specifically, the regression, classification, and novel grid optimization experiments.

\subsection{Regression Results Discussion}
\cite{jena2012prediction} achieved an \ac{RMSE} of \~175m in their optimized model.
Their model used \ac{ML} to predict an optimum scaling factor \textbf{S}.
The linear regression model fit in this project is 100 meters less accurate than the optimized model used in \cite{jena2012prediction}.
The purpose of the test is not to achieve accurate predictions, but to identify if \ac{ML} models can be viable for predicting bathymetry.
Therefore, the accuracy of these models is less important than identifying the viability of the models.
The training data used is predicted bathymetry, but shows that fitting a model to true bathymetry will yield a similar result.

\subsection{Classification Results Discussion}
The Random Forest model excelled with a balanced accuracy of 82\%.
Breaking down the results by class, the classifier predicted some classes with greater precision than others.
This indicates that the decision boundary responded to certain trends in the data.
In general, the Random Forest classifier performed better overall, which is why it was the best performing classifier for 47.2\% of the world.
This percentage can be seen in Figure~\ref{fig:pie_best_fit}.

\par
The Bagging classifier performed on par with the Random Forest Classifier with a balanced accuracy of 79\%.
However, Figure~\ref{fig:coveragegrid} shows that the bagging classifier performed best around coastlines.
This suggests that the model responds well to shallow waters.
Shoreline data will also be consistently more accurate because of the proximity to land.
Most of the world's high-resolution bathymetry is shoreline data, and it is possible that the model responded to the quality data.


\par
The classification results show that labeling bathymetry can improve the performance of the models.
In this work, it was tested that a random forest classifier can predict 82\% of the world's predicted bathymetry within 150 meters of accuracy. 
However, the more interesting topic to analyze is the behavior of the models.
The data used in training comes from aggregated external datasets and a predicted bathymetry dataset.
The predictions of these models are being compared to predicted bathymetry, which represents the accuracy with relation to predicted values.
This means that the accuracy in these models is not indicative of truly predicting global bathymetry.
It does show that an \ac{ML} model can be fit to data and be used to predict bathymetry, and if actually measured bathymetry and ocean features were used in training, the results would be comparable.
Furthermore, parameter tuning, model selection, and dynamic feature selection could be used the increase the accuracy beyond current results.

\par
Interestingly, some models excel along fault lines.
For example, the decision tree classifier in Figure~\ref{fig:coveragegrid} performs well along what appear to be fault lines.
However, when making predictions in the global scope the classifier is very inaccurate.
This suggests that a classifier can be optimized using geospatial position.



\subsection{Grid-Optimized Model Discussion}
%The world wide ETOPO bathymetry dataset \cite{national1988etopo} at two minute resolution is used for valadation and metrics.
%This dataset is treated as the ground truth for all predictions.
%During the experiment, a one third holdout was used for validation in some cases.
%For finding the optimum model for a coverage a 10 fold cross validation was utilized.

%Include metrics information here.... possibly graphs and a list of scores? I dont really know.

%I dont like how I worded this whole section...
%The idea here is that I want to say "Hey, these people did this research and found the their regression model preforms poorly for predicting seamounts espicially after 500m.
%I clearly noticed a similar trend, but saw better preformance from some models than others. 
%This research is to identify those coverages and then use those results to build a super classifier.
%These metrics are important for identifying where the models preform well. 
\par
Grid optimization improved the accuracy of predictions by \~{}5\%.
These results are displayed in Figure~\ref{fig:grid_opt_graph}.
Obviously, the model selection and subsequent injection improved the results, creating an ensemble of many models and selecting them on demand.
This could be caused by geophysical characteristics that benefit one classifier.
For example, in Figure~\ref{fig:coveragegrid}, the Decision Tree classifier performed best along what appears to be fault lines.
It is possible that the characteristics related to being in proximity to a fault line benefited the model's decision boundary. 

\par
Clearly, Figure~\ref{fig:coveragegrid} provides evidence that model decision boundaries are sensitive to the features based on location.
This leads to the theory that there is not a single best fit model for predicting global bathymetry.
Analysis of Figure~\ref{fig:coveragegrid} shows interesting consistencies that raise questions about the underlying features.
For example, in Figure~\ref{fig:coveragegrid}, the Bagging classifier appears to perform best around coast lines.
This is consistent for most of the globe.
However, in some coastal areas the Random Forest Classifier performs best. 
It is possible these areas are linked to port cities and the high-resolution bathymetry collected from shipping lanes.

\par
Another interesting consistency is around fault lines.
The coverages where the decision tree classifier performed best seems to follow fault lines.
It is possible there is a geospatial attribute that contributes to this success, or a specific feature in these locations that contributes to the decision tree classifier's performance.
The AdaBoostClassifier also performs well around fault lines, but to a lesser extent.
It is possible there are trends in the data that explain why the AdaBoostClassifier shows this behavior.

\begin{figure}[htp]
    \centering
    \includegraphics[width=\textwidth]{opt_grid_bar.png}
    \caption[Graph depicting average balanced accuracy of each model in cache]{Average prediction accuracy of coverages where a model performed well. 
    GBC had the highest average accuracy, but had a very small coverage.}
    \label{fig:grid_opt_graph}
\end{figure}

\par
Figure~\ref{fig:grid_opt_graph} represents the average prediction accuracy across all coverages where a model was the best-fit
, which means that the model had the highest accuracy for predictions in that coverage.
It is important to note that this is representing models that were trained on a reduce set of data, therefore, it is expected for the accuracy to be lower.
What this graph shows is that the Random Forest Classifier had an average prediction rate of 51\% across coverages.
This is fairly consistent with what was expected as it was the best performing global model. 
However, other models that performed very poor on a global scale made accurate predictions in specific areas.
For example, the Quadratic Discriminant Analysis classifier performed exceptionally well in specific areas, as well as the Ada Boost Classifier and the Artificial Neural Network classifier.
With this being noted, selecting a optimal grid for prediction will allow the strongest classifier to be used at all times.

% \begin{table}[htp]
%     \centering
%     \begin{tabular}{|c c c|}
%         \hline
%         \textbf{Model} & \textbf{Average F1 Score} & \textbf{Mean Balanced Accuracy} \\
% 		\hline
% 		Grid Optimized Model Injector & 0.83 & 0.862 \\
% 		\hline
%     \end{tabular}
%     \label{table:GRID_OPT_RESULTS}
%     \caption{Grid Optimized Model Injector Results}
% \end{table}

\par
The evidence shows that an \ac{ML} model for predicting bathymetry could be chosen based on geospatial location.
Future work may include investigating the appropriate feature sets, coverage boundaries, depth boundaries, parameters, and metrics based on geophysical location.
For example, volcanic activity creates new land.
This activity has a causal relationship to bathymetry, but volcanic activity at a specific point may not affect the bathymetry at a potential antipodal point.
This is another example that the coverage boundaries are potentially a naive selection choice.
It is possible that choosing models by performance across depth boundaries proves to be a better selector.
%Extending off the research preformed in \cite{jena2012prediction} these metrics allow the selection of the best preforming model.
%This is where I define what happens after the appropriate coverages have been found.
\par
In this research, this optimization allowed each model to perform to its optimum.
Each coverage highlighted distinct characteristics that could produce a better decision boundary.
These coverages are simple partitions of the world, but could be extended in future work to optimize the selection.
% Reconsider this sentence....



%Maybe here I can have a table show casing the preformance of models or possibly statistics about the coverages??
%It will be intersting to see what has preformed better across the globe
%Also is intersting to see which models have preformed best overall.

\subsection{General Discussion}
The predictions made in this work are based on predicted bathymetry.
Even with an 85\% prediction rate, these models are not fit to compare with \acf{EGM}.
The experiments in this work do show that there are accuracy gains to be achieved with model selection.
Figure~\ref{fig:coveragegrid} gives evidence that there is not a best fit model for predicting bathymetry globally.
Figure~\ref{fig:grid_opt_graph} gives evidence that an ensemble of some kind, optimizing by a decision function, will likely yield better results.

The grid optimization for model selection performed in this work is a novel approach for predicting bathymetry and provides evidence for optimal model selection with regards to predicting global bathymetry.
It is reasonable to assume that selecting a model with a decision function will produce better predictions.
Identifying the optimal decision function will help to explain why different models perform better or worse over the coverages.
Grid optimization shows that a simple geospatial decision function can be used to improve theoretical prediction accuracy.