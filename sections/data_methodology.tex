\section{Data Analysis}
\setlength{\parindent}{10ex}
The data for this paper was aggregated from many studies.
The ETOPO study was used for the bathymetry readings \cite{national1988etopo}.
Other features were aggregated from several studys.
See figure below for a list of features used and correponding origin study. 

%This section is for defining the data being used in the experiment
%It should define where the data came from and its format
%I can also explain any of the special stuff I am doing (Binning for example)
%

\begin{table}[htb]
    \begin{tabular}{ |c|c| }
        \hline
            \textbf{Feature} & \textbf{Origin Study} \\
            Mantle Density & CRUST1 \cite{laske2013update} \\
            LAND One Hot & ETOPO \cite{national1988etopo} \\
            Crust Thickness & CRUST1 \cite{laske2013update} \\
            Low, Mid, High Crust Density & CRUST1 \cite{laske2013update} \\
            Estimated Current East, North, Mag & HYCOM \cite{chassignet2009us} \\
            Sea Nitrate, Phosphate, Salinity, Silicate Measurements & WOA \\
            Sea Temperate & WOA \\
            Sediment Thickness & CRUST1 \cite{laske2013update} \\
            BioMass Features & Goyetx \\
            Geoid Features & EGM \cite{pavlis2008earth} \\
            Wave height, period & WAVEWATCH \cite{tolman20072007} \\
        \hline
    \end{tabular}
\end{table}

%I am using this section to introduce the feature selection method that I preformed
%Maybe I can flesh this out more and talk about it in def??
%Maybe make a diagram for the flow of the GA?
\subsection{Feature Selection}
A gentic algorithm was used for feature selection \cite{yang1998feature}.
The initial population was created using one hots to represent the space of features.
A random number generator assigned each member a set of the potential features. 
Each member was then used to train a model and the resulting balanced accuracy of that model represented a fitness level.
The top two percent are then moved to the next generation.
While the rest of the top forty percent are randomly paried to produce offspring.
A cross over mutation is applied using the multi point cross over method.
A mutation is then applied to one percent of the population.

\par
This algorithm terminates when the balanced accuracy of a model exceeds a threshold.
The highest ranked member of the population is then chosen as the selected features.
The result of my implementation of this algorithm are documented in figure...

%This decribes how the grid files are used and oragnized...
%They are essentially binary files...
\subsection{Data Representation}
The data used in this project is organized into cell centered grids.
Where each grid cell value represents a average value across the cell.
Each grid represents a cell that maps to the EPSG:3857 coordinate reference system.
% this sentence below seems weird but meh?
The grids have a resolution which defines the number of cells along an axis.
High resolutions grids provide a more accurate coverage at the cost of memory.

\par
Data used in this project is fit into two minute bathymetry grid. 
This was done to minimize the interpolation required to fit higher resolution grids.
While also maximizing the original studies accuracies and avalaible resources.
However, some studies offer data at a smaller resolution than what is used in this project.
See the SRMTM30 \cite{becker2009global} for an example of such a study.

\subsection{Ocean Features}
This effort utilizes aggregated ocean features from other projects to predict bathymetry.
The most current efforts in predicting bathymetry are gravitational models that use sattelite altimeter data as a feature.
A related work \cite{jena2012prediction} uses machine learning models with altimeter data to as a feature to programatically find a regression line.
This effort is focussed on using a collection of ocean features as opposed to just sattelite altimetry.

\par
These ocean features are aggregated into corresponding grid files.
The initial set of features was randomly chosen without a speicific purpose.
All studies that potentially yielded helpful features were used to build the set.
Correlation tests were ran as experiments to see how the set of data will correlate.

%Maybe I can talk about some of the correlations I preformed here???? A few graphs perhaps???
%Maybe talk about the PCC???
%Possibly need to talk about the breadth of features here....

\subsection{Bathymetry Values}
The bathymetry data used for training are extracted from the ETOPO study \cite{national1988etopo}.
This data is a aggregation of predictions from gravitational models \cite{smith1997global} \cite{smith1994bathymetric}, and Multi Beam Echo Sonar (MBES) readings \cite{farr1980multibeam}.
MBES readings are accurate and reliable. 
Naturaly, it is cost and time prohibitive for a set of ships to survey the entire world.
The ETOPO dataset uses predicted values from gravitational models to fill gaps in the coverage.

%There is a better was to describe why the features were binned like this
%I want to describe the value that was added by binning my data into classes!
\par
The bathymetry used for training was binned into classes for classification.
These classes where partioned on a interval of 150 meters.
This partitioning scheme was chosen to improve upon the results from a similar project \cite{jena2012prediction}.
The binning is preformed to leverage the unique advantages inherient to classification problems.
For example, the use of metrics such as a confusion matrix makes comparing the preformance of the model signifigantly easier.



